# RLPD Configuration for Mock Environment Testing
# RLPD = Reinforcement Learning with Prior Data
# This configuration is for testing the RLPD pipeline without real hardware

defaults:
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null

# Runner configuration
runner:
  task_type: rlpd
  logger:
    log_path: "./results/rlpd_mock"
    project_name: rlinf_rlpd
    experiment_name: "rlpd_mock_test"
    logger_backends: ["tensorboard"]

# RLPD specific configuration
rlpd:
  # Environment
  max_episode_steps: 200
  
  # Replay buffers
  replay_buffer_capacity: 50000
  demo_buffer_capacity: 10000
  min_buffer_size: 500
  
  # Training
  batch_size: 128
  learning_rate: 3.0e-4
  gamma: 0.99
  tau: 0.005
  
  # SAC
  init_temperature: 1.0
  target_entropy: null  # Will be set to -action_dim
  backup_entropy: true
  
  # Update schedule
  utd_ratio: 4
  critic_updates_per_actor: 1
  
  # RLPD-style sampling (50/50 from online and demo buffers)
  demo_ratio: 0.5
  
  # Model architecture
  hidden_dims: [256, 256]
  encoder_type: "mlp"  # Use MLP for mock testing
  image_keys: []
  use_proprio: true
  
  # Input device
  input_device: "mock"  # Use mock device for testing
  
  # Logging
  log_interval: 10
  eval_interval: 500
  save_interval: 1000
  
  # Paths
  checkpoint_dir: "./results/rlpd_mock/checkpoints"
  buffer_dir: "./results/rlpd_mock/buffers"

# Mock environment configuration (for testing)
env:
  simulator_type: "mock"
  obs_dim: 14  # tcp_pose (7) + tcp_vel (6) + gripper (1)
  action_dim: 7
  max_steps_per_episode: 200

